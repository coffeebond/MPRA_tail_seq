'''
This script has two modes:
> Mode 1 takes three files
1. STAR-aligned bam file
2. read 2 fastq file as UMI
3. intensity file from read1

> Mode 2 takes 1 file
1. A_signal output file generated by this script

Note: try to use multiprocessing for faster speed

It calculates poly(A) tail length based on a Gaussian mixture hidden markov model trained on a subset of the data
It outputs these files:
1. "*_log.txt", a log file for the run
2. "*_pA_all_tags.txt", all clusters (tags) containing: 
3. "_states.txt", a file (to be zipped later) containing:
4. "_HMM_model.txt", the HMM model 
5. A temporary file containing converted A-signal will be written out on the disk to save memory usage
	and it can be deleted in the end. (only in mode 1)

###--- v1.1 change log:
	1. Added an input option "pm", which can be used to change the global parameter "pa_max"
	2. Changed the input option "p" to "ps".
	3. Added an input option "a", which when specified allows the script to annotate each read (default is not to annotate)
	4. Added an input option "m", which specifies a pre-trained HMM model for direct prediction

###--- v1.2 change log:
	1. Fixed the barcode length 
	2. Fixed A-signal mode (A signal file needs to be unziped)

###--- v1.3 change log (20240815):
	1. Added a global parameter "channel_order" to be compatible with both HiSeq and AVITI.
	2. Added a funciton "get_tl_from_fastq" to obtain tail length direction from the fastq file
	3. Re-wrote the code that determines tail length
'''


import sys, subprocess, math, gzip, ghmm, time, tarfile, concurrent.futures, random, os, argparse, pysam
import numpy as np
from ghmm import *
from time import time
from datetime import datetime
from collections import OrderedDict
from Bio import Align

###------global variables-----------------------------------------------------------------
params = OrderedDict([
	('seq_3p', 'CACTCTTTCCCTACACGACG'), # 3' adapter sequence
	('seq_3p_alignment_score_cutoff', 15), # alignment score cutoff 
	('chunk', 1000000),
	('n_threads', 20),
	('chunk_lines', 12500), # number of lines to allocate to each core to process, if too big, memory may fail),
	('training_max', 50000),
	('training_min', 5000),
	('nor_pos_start', 1), # (1-based) starting position of the region used for normalization
	('nor_pos_end', 100), # (1-based) ending position of the region used for normalization
	('pa_start_pos', 101), # (1-based) starting position of the poly(A)
	('pa_max', 180), # the maximal length of a poly(A) tail that can be sequenced in a run (depends on the number of cycles)
	('channel_order','ACGT'), # order the channel in the intensity file; "ACGT" for HiSeq and 'CAGT' for AVITI
	('intensity_start_pos', 5), # (1-based) starting position of the intensity data in the intensity file for each row (4 or 5 for HiSeq, 1 for AVITI)
	('all_zero_limit', 5), # limit for total number of all zeros in four channels in the intensity file
	('bound', 5), # boundary for normalized log2 A_signal
	('allow_back', True), # whether to allow bi-directional state transition
	('mixed_model', True), # whether to use mixed gaussian model for the input variable
	#('pA_call_mode', 1), # use 1 or 2
	('non_A_allowed', 4) # number non-A bases allowed at the beginning the poly(A) tail region.
])


###------functions------------------------------------------------------------------------
def fread(file): # flexibly read in files for processing
	file = str(file)
	if file.endswith('tar.gz'):
		temp = tarfile.open(file, 'r:gz')
		return temp.extractfile(temp.next())
	elif file.endswith('.gz'):
		return gzip.open(file, 'rt')
	elif file.endswith('.txt'):
		return open(file, 'rt')
	else:
		sys.exit("Wrong file type to read: " + file)

def timer(): # calculate runtime
	temp = str(time()-t_start).split('.')[0]
	temp =  '\t' + temp + 's passed...' + '\t' + str(datetime.now())
	return temp

def pwrite(f, text): # a decorator for print; it prints and also writes the content to a log file
	f.write(text + '\n')
	print(text)

def make_log_file(filename, p_params = False, p_vars = False):
	f_log = open(filename, 'w')
	if isinstance(p_params, dict):
		pwrite(f_log, 'Global parameters:')
		for param in p_params:
			pwrite(f_log, param + ': ' + str(p_params[param]))
	if isinstance(p_vars, dict):
		pwrite(f_log, '\nInput arguments:')
		for var in p_vars:
			pwrite(f_log, var + ': ' + str(p_vars[var]))
		pwrite(f_log, '\n')	
	return(f_log)

def get_tl_from_fastq(seq, ref_seq = params['seq_3p'], score_cutoff = params['seq_3p_alignment_score_cutoff']):
# this function obtains tail length directly from a fastq file
	seq_pA = seq[(params['pa_start_pos']-1):]
	
	aligner = Align.PairwiseAligner(mode = 'local')
	aligner.match_score = 2
	aligner.mismatch_score = -3
	aligner.open_gap_score = -5
	aligner.extend_gap_score = -2
	aligner.target_end_gap_score = 0.0
	aligner.query_end_gap_score = 0.0

	alignments = aligner.align(seq_pA, ref_seq)

	# by default, use the length of the longest stretch of As in the fastq file
	mode = 'by_fastq'

	if alignments.score >= score_cutoff:

		try:
			if len(alignments) > 0 and len(alignments) < 10:
				# if the 3' adpater can be located (passing the alignment score cutoff), use the location to determine the tail length
				mode = 'by_adapter'
		except:
			pass
		#	pwrite(f_log, 'Reference sequence: ' + ref_seq)
		#	pwrite(f_log, 'Target sequence: ' + seq_pA)
		#	pwrite(f_log, 'Alignment score: ' + str(alignments.score))

	if mode == 'by_adapter':
		# use the first alignment (most 5'-end one) to determine the relative position between the sequence and the adapter
		for alignment in alignments:
			#print(alignment)
			tl = alignment.aligned[0][0][0] - alignment.aligned[1][0][0]
			break

	else:
		# pad the poly(A) region with two non-A bases at the 3' end to ensure poly(A) counting will end 
		seq_pA = seq[(params['pa_start_pos']-1):] + 'CC'

		# calculate the tail length
		flag_start = False
		
		for i in range(len(seq_pA)-1):
			if i >= params['non_A_allowed']:
				if flag_start == False:
					if 'A' in seq_pA[:i]:
						tl = 1
					else:	
						tl = 0 
					break
				else:
					if seq_pA[i] != 'A' and seq_pA[i+1] != 'A': # need two non-A bases to stop counting tail length
						tl_end = i
						tl = tl_end - tl_start
						break
			else:
				if flag_start == False:
					if seq_pA[i] == 'A' and seq_pA[i+1] == 'A': # need two As to start counting tail length
						tl_start = i
						flag_start = True
				else:
					if seq_pA[i] != 'A' and seq_pA[i+1] != 'A': # need two non-A bases to stop counting tail length
						tl_end = i
						tl = tl_end - tl_start
						break
	try:
		return(tl, mode)
	except:
		pwrite(f_log, seq_pA)
		sys.exit("Can't determine the tail length by the sequence in the fastq file for this sequence: " + seq_pA)

def Convert2A(line):
# this function does two things:
# 1. normalize intensity for all four channels of each cluster, using the 3' UTR 
	dict_value = {'A':[] ,'C':[] ,'G':[], 'T':[]}
	
	
	# for HiSeq, getting the index and the intensity list
	lst = line.rstrip().split('\t') 
	idx = ':'.join(lst[:3])
	'''

	# for AVITI, getting the index and the intensity list
	idx = ':'.join(line.rstrip().split('\t')[0].split(' ')[0].split(':')[-3:]) 
	lst = [' '.join(line.rstrip().split('\t')[3].split(' ')[i*4 : (i+1)*4]) for i in range(int(len(line.rstrip().split('\t')[3].split(' '))/4))]
	'''

	for i in range(max(0, (params['nor_pos_start'] - 1)), params['nor_pos_end']): # i is the index of the i+1 base in the random region 
		lst4c = list(map(int, [x for x in lst[i+params['intensity_start_pos']-1].split(' ') if x != '']))
		# first 4 elements are not intensities
		# get rid of spaces between intensity values
		dict_4c = {'A':lst4c[params['channel_order'].index('A')],
				   'C':lst4c[params['channel_order'].index('C')],
				   'G':lst4c[params['channel_order'].index('G')],
				   'T':lst4c[params['channel_order'].index('T')]}
		base = mdict[idx]['seq'][i - max(0, (params['nor_pos_start'] - 1))] # the index in the intensity line is different from the index in the random sequence in the dictionary
		if base in dict_value:
			dict_value[base].append(dict_4c[base]) 
	for key in dict_value:
		if len(dict_value[key]) == 0 or np.mean(dict_value[key]) <= 0:
			return None
			# exit this function if normalization can't be completed
		else:
			dict_value[key] = np.mean(dict_value[key])
			# otherwise, take the average value
			# this should be the approximate value illumina used to call the base for that cluster
# 2. convert intensity from 4 channels to A signal
	all_A = []
	temp_pos_offset = 1 # note in the 20240828 dataset, the start position needs to be shifted +1 due to repeat of cycle 26
	for j in range(params['pa_start_pos']-1+params['intensity_start_pos']-1 + temp_pos_offset, params['pa_start_pos']-1+params['intensity_start_pos']-1 + params['pa_max']): # j is the index of j+1 position in intensity line
		lst4c = map(int, [x for x in lst[j].split(' ') if x != ''])
		if lst4c == [0,0,0,0]:
			all_A.append('empty')
		# sometimes in a base position, all channel signals equal to 0
		# these need to be corrected later or discarded if there are too many in a cluster
		else:
			dict_4c = {'A':lst4c[params['channel_order'].index('A')],
					   'C':lst4c[params['channel_order'].index('C')],
					   'G':lst4c[params['channel_order'].index('G')],
					   'T':lst4c[params['channel_order'].index('T')]}
			for key in dict_4c:
				if dict_4c[key] <= 0:
					dict_4c[key] = abs(1.0 / dict_value[key])
				else:
					dict_4c[key] = float(dict_4c[key]) / dict_value[key]
				# normalize the intensity value 
			A_signal = dict_4c['A'] / (dict_4c['T'] + dict_4c['C'] + dict_4c['G'])
			A_signal = math.log(A_signal, 2)
			A_signal = max(-params['bound'], min(A_signal, params['bound']))
			# make large or small A_signal bound
			all_A.append(A_signal)
	if all_A.count('empty') >= params['all_zero_limit']:
		return None
	else:
		for k in range(len(all_A)):
			if all_A[k] == 'empty':
				sliding_A = all_A[max(0, k - params['all_zero_limit']):min(len(all_A), k + params['all_zero_limit'])]
				sliding_A = [x for x in sliding_A if x != 'empty']
				all_A[k] = np.mean(sliding_A)
		# if there more than all_zero_limit base positions with all channel signals being equal to 0, discard this cluster
		# else, use the mean in a sliding window to fill in the missing value
	all_A.insert(0, params['bound']*100) # add a pseudo A to the front, making sure HMM starts with A
	return all_A

def worker_C2A(lines):
# this function takes all lines from a intensity file and allocates them to each process
# and outputs a tupple including
# 1. a new dictionary which contains converted T-signal
# 2. a list containing converted A-signals to be trained
	temp_dict = {}
	temp_lst = []
	for line in lines:	
		
		idx = ':'.join(line.rstrip().split('\t')[:3]) # for HiSeq 
		#idx = ':'.join(line.rstrip().split('\t')[0].split(' ')[0].split(':')[-3:]) # for AVITI
		
		if idx in mdict:
			temp = Convert2A(line)
			if temp: # only reads that have converted T-signal will be used later			
				temp_dict.setdefault(idx, temp)
				if idx in train_keys:
					temp_lst.append(temp)
	return temp_dict, temp_lst

def worker_hmm(lines):
# this function takes a number of lines containing converted A-signal and calculates the tail length 
# It returns:
# a dictionary {id/gene_name : {tl: tail_length, states: HMM states}}
	temp_dict = {}
	temp_lst = []
	for line in lines:
		info, A_signal = line.rstrip().split('\t\t')
		states = model.viterbi(EmissionSequence(F, map(float, A_signal.split('\t'))))[0]
		# tail length defined by the position followed by two non-A bases (state 3 and 4)
		# Also, the first position is a peudo-T base
		if len(states) == 0:
			sys.exit("Can't infer the states from the model! Exiting...")

		# convert the sequence to A (+1) or non-A (-1) state and pad two non-A states in the end
		a_lst = [1 if x <= ((len(pi) - 1) / 2) else -1 for x in states] + [-1, -1] 

		# calculate the tail length
		flag_start = False
		for i in range(1, len(a_lst)-1): # skip the first pseudo-A base
			if i > params['non_A_allowed']:
				if flag_start == False:
					if 1 in a_lst[1:i]:
						tl = 1
					else:	
						tl = 0 
					break
				else:
					if a_lst[i] != 1 and a_lst[i+1] != 1: # need two non-A bases to stop counting tail length, which will definite happen in the end of the list
						tl_end = i
						tl = tl_end - tl_start
						break
			else:
				if flag_start == False:
					if a_lst[i] == 1 and a_lst[i+1] == 1: # need two As to start counting tail length
						tl_start = i
						flag_start = True
				else:
					if a_lst[i] != 1 and a_lst[i+1] != 1: # need two non-A bases to stop counting tail length
						tl_end = i
						tl = tl_end - tl_start
						break

		temp_dict.setdefault(info, {'tl': str(tl), 'states': str(states)})
	return temp_dict

def lines_sampler(filename, n_lines_sample):
	# this function takes in the reads_wTsignal file and
	# randomly select n_lines_sample lines to output as a list of lists (each containing T signals)
	sample = []
	with fread(filename) as f:
		f.seek(0, 2)
		filesize = f.tell()
		random_set = sorted(random.sample(xrange(filesize), n_lines_sample))
		for i in range(n_lines_sample):
			f.seek(random_set[i])
			
			# Skip current line (because we might be in the middle of a line) 
			f.readline()
			
			# Append the next line to the sample set 
			line = f.readline()
			if line:
				sample.append(list(map(float, line.rstrip().split('\t\t')[-1].split('\t'))))
	return sample
	

#####################################################################################################################
###------the script runs from there-------------------------------------------------------
t_start = time() # timer start

# parse the input
parser = argparse.ArgumentParser()
parser.add_argument('-b', '--bam', dest = 'b', type = str, help = 'aligned bam file')
parser.add_argument('-fq', '--fastq', dest = 'fq', type = str, help = 'input fastq file or the read with poly(A) region')
parser.add_argument('-u', '--umi', dest = 'u', type = str, help = 'input fastq file for the UMI read')
parser.add_argument('-i', '--intensity', dest = 'i', type = str, help = 'input intensity file')
parser.add_argument('-s', '--signal', dest = 's', type = str, help = 'input A signal file')
parser.add_argument('-m', '--model', dest = 'm', type = str, help = 'pre-defined HMM model file')
parser.add_argument('-ps', '--pa_start', dest = 'ps', type = int, help = 'starting position for poly(A) region')
parser.add_argument('-pm', '--pa_max', dest = 'pm', type = int, help = 'the maximal length of a poly(A) tail')
parser.add_argument('-a', '--annotate', dest = 'a', action='store_true', help = 'whether to annotate each mapped read')
parser.add_argument('-o', '--out', dest = 'o', type = str, help = 'output file prefix')
args = parser.parse_args()  

if args.ps:
	params['pa_start_pos'] = args.ps
if args.pm:
	params['pa_max'] = args.pm

# check input files and determine which mode to run
if args.s:
	hmm_only = True
else:
	if args.b and args.i:
		hmm_only = False
	else:
		sys.exit('If "-s" is not specified, both "-b" and "-i" must be provided in the input.')

###-------------------------------------------------	
if not hmm_only:  
	# processed the data if an A_signal file is not provided

	if args.o:
		out_prefix = args.o
	else:
		out_prefix = args.b.split('/')[-1].split('.')[0]

	f_log = make_log_file(out_prefix + 'complete_log.txt', p_params = params, p_vars = vars(args))
	
	'''
	###------------------------------------------------
	get mapped read from the sorted and index bam file
	dictionary:
	if annotate:
		{read_index:{chrmosome(gene), variant_type, position, cigar, md_tag, read_seq, lowest_quality_score_read, quality_score_of_the_substitution}}
	else:
		{read_index:{chrmosome(gene)}}
	'''
	pwrite(f_log, '\nProcessing the bam file...')
	mdict = {}
	bam_file = pysam.AlignmentFile(args.b)
	for i, line in enumerate(bam_file):
		
		idx = ':'.join(line.query_name.split('#')[0].split(':')[-3:]) # parse data from HiSeq
		#idx = ':'.join(line.query_name.split('#')[0].split(':')[-4:-1]) # parse data from AVITI

		if args.a: # annotate each mapped read
			read = line.query_sequence
			gene = line.reference_name
			pos = line.reference_start + 1 # convert to 1-based position
			cigar = line.cigarstring
			md = line.get_tag('MD')
			
			qs_lst = [int(j) + 33 - 64 for j in line.query_qualities] # for illumina 1.5+, pysam thinks the offset is 33, but for HiSeq, the offset is 64
			#qs_lst = [int(j) for j in line.query_qualities] # for AVITI, no correction is needed
			
			qs_min = min(qs_lst) 
			
			# get the the quality score of single-nucleotide substitution
			qs_mm = 'NA'
			if cigar == '100M' and int(pos) == 1:
				if md == '100':
					variant = 'WT'
				elif 'N' in md:
					variant = 'other'
				else:
					mt_pos = int(re.split(r'[ACGT]', md)[0]) + 1 # 1-based
					mt_nt = read[mt_pos - 1]
					wt_nt = re.sub(r'^\d+', '', md)[0]
					variant = wt_nt + str(mt_pos) + mt_nt
					qs_mm = qs_lst[mt_pos - 1]
			else:
				variant = 'other'

			if idx in mdict:
				pwrite(f_log, 'Duplicated reads in the bam file!')
				sys.exit()
			else:
				mdict[idx] = OrderedDict([
					('gene', gene), 
					('variant', variant), 
					('pos', pos), 
					('cigar',cigar), 
					('md', md), 
					('seq',read), 
					('qs_min',qs_min),
					('qs_mm', qs_mm)
					])
		else:
			mdict[idx] = OrderedDict([
				('gene', line.reference_name),
				('seq', line.query_sequence)
				])

		if (i+1) % params['chunk'] == 0:
			pwrite(f_log, str(i+1) + ' reads have been processed...' + timer())
	pwrite(f_log, str(i+1) + ' reads have been processed...'  + timer()) 
	pwrite(f_log, 'Number of mapped reads in the bam file: ' + str(i+1))

	
	###------------------------------------------------
	'''
	get the umi from read 2
	dictionary:
	if annotate:
		{read_index:{chrmosome(gene), variant_type, position, cigar, md_tag, read_seq, lowest_quality_score_read, quality_score_of_the_substitution, lowest_quality_score_read, umi, lowest_quality_score_umi}}
	else:
		{read_index:{chrmosome(gene), lowest_quality_score_read, umi, lowest_quality_score_umi}}
	'''

	pwrite(f_log, '\nObtaining the umi...')
	f = fread(args.u)
	i = 0
	while(True):
		line = f.readline()
		if not line:
			break
		else:
			idx = ':'.join(line.split('#')[0].split(':')[-3:]) # for HiSeq
			#idx = ':'.join(line.split(' ')[0].split(':')[-4:-1]) # for AVITI
			
			# umi = f.readline().rstrip()[:-1] # not including the last nucleotide for HiSeq (it depends)
			umi = f.readline().rstrip()

			f.readline()

			qs_min = str(min(ord(q) - 64 for q in f.readline().rstrip()[:-1])) # for HiSeq
			#qs_min = str(min(ord(q) - 33 for q in f.readline().rstrip()[:-1])) # for AVITI

			if idx in mdict: 
				mdict[idx].update([('umi',umi), ('umi_qs_min',qs_min)])
			i += 1
			if (i+1) % params['chunk'] == 0:
				pwrite(f_log, str(i+1) + ' reads have been processed...'+ timer()) 
	pwrite(f_log, str(i+1) + ' reads have been processed...'+ timer()) 

	
	###------------------------------------------------
	'''
	determine poly(A) tail length based on read1 fastq file (if provided)
	dictionary:
	if annotate:
		{read_index:{chrmosome(gene), variant_type, position, cigar, md_tag, read_seq, lowest_quality_score_read, quality_score_of_the_substitution, lowest_quality_score_read, umi, lowest_quality_score_umi, tl_fq, tl_fq_mode}}
	else:
		{read_index:{chrmosome(gene), lowest_quality_score_read, umi, lowest_quality_score_umi, tl_fq, tl_fq_mode}}
	'''
	
	if args.fq:
		pwrite(f_log, '\nPoly(A) fastq file is provided. Use this to determine poly(A) tail length...')
		f = fread(args.fq)
		i = 0
		j = 0
		k = 0
		while(True):
			line = f.readline()
			if not line:
				break
			else:
				idx = ':'.join(line.split('#')[0].split(':')[-3:]) # for HiSeq
				#idx = ':'.join(line.split(' ')[0].split(':')[-4:-1]) # parse data from AVITI
				
				seq = f.readline().rstrip()
				f.readline()
				f.readline()

				if idx in mdict:
					k += 1
					tl_fq, tl_fq_mode = get_tl_from_fastq(seq = seq)
					mdict[idx].update([('tail_length_fq', tl_fq), ('tail_length_fq_mode', tl_fq_mode)])
					if tl_fq_mode == 'by_adapter':
						j += 1

				i += 1
			if i % params['chunk'] == 0:
				pwrite(f_log, str(i) + ' reads have been processed...'+ timer()) 
				pwrite(f_log, '\tOf ' + str(k) + ' aligned reads, ' + str(j) + ' had adapters found.')
		pwrite(f_log, str(i) + ' reads have been processed...'+ timer()) 
		pwrite(f_log, '\tOf ' + str(k) + ' aligned reads, ' + str(j) + ' had adapters found.')

	###------------------------------------------------
	# randomly pick a set for HMM training
	pwrite(f_log, '\nRandomly picking training set:')
	n_train = min(max(int(len(mdict)/100), params['training_min']), params['training_max'])
	train_keys = random.sample(list(mdict.keys()), n_train)
	pwrite(f_log, str(n_train) + ' reads picked for training...')

	###------------------------------------------------
	# read intensity file and convert 4-channel intensities to single log-transformed bound A signal
	# output A signals to a file

	pwrite(f_log, '\nReading the intensity file...' + timer())	
	counting = 0
	counting_out = 0
	train_set = []
	line_lst = []
	fi = fread(args.i)
	out_A_signal = out_prefix + 'A_signal.txt'
	outA = open(out_A_signal, 'w')
	for idx in mdict: # write the column values
		outA.write('idx' + '\t' + '\t'.join(list(map(str, mdict[idx].keys()))) + '\t\t' + 'A signals' + '\n')
		break

	while(True):
		line = fi.readline()
		if not line:
			with concurrent.futures.ProcessPoolExecutor(params['n_threads']) as pool:
				futures = pool.map(worker_C2A,[line_lst[n:n+params['chunk_lines']] for n in range(0,len(line_lst),params['chunk_lines'])])
				for (d,l) in futures: # combine data from outputs from all processes
					train_set.extend(l)
					for key in d: # write converted A-signal to a file
						outA.write(key + '\t' + '\t'.join(list(map(str, mdict[key].values()))) + '\t\t' + '\t'.join(list(map(str, d[key]))) + '\n')
						counting_out += 1
				pwrite(f_log, str(counting) + ' reads processed...' + timer())
			break
		else:
			line_lst.append(line)
			counting += 1
			if counting % (params['chunk_lines'] * params['n_threads']) == 0:
				with concurrent.futures.ProcessPoolExecutor(params['n_threads']) as pool:
					futures = pool.map(worker_C2A,[line_lst[n:n+params['chunk_lines']] for n in range(0,len(line_lst),params['chunk_lines'])])
					for (d,l) in futures: # combine data from outputs from all processes
						train_set.extend(l)
						for key in d: # write converted A-signal to a file
							# use a double tab to seperate the information and the A signals
							outA.write(key + '\t' + '\t'.join(list(map(str, mdict[key].values()))) + '\t\t' + '\t'.join(list(map(str, d[key]))) + '\n')
							counting_out += 1
				line_lst = []

			if counting % params['chunk'] == 0:
				pwrite(f_log, str(counting) + ' reads processed...' + timer())
				

	pwrite(f_log, 'The number of reads in training set after intensity-conversion: ' +  str(len(train_set)))
	pwrite(f_log, 'The total number of reads after intensity-conversion: ' + str(counting_out))
	pwrite(f_log, 'Finished processing the intensity file...' + timer())
	fi.close()
	outA.close()
	mdict.clear() # clear the dictionary to free up some memory

else:
	# get the processed data if an A_signal file is provided
	if not os.path.isfile(args.s):
		sys.exit('Error! No Asignal file found!')
	if args.s.endswith('gz'):
		sys.exit('Error! The Asignal file needs not to be zipped!')
	if args.o:
		out_prefix = args.o
	else:
		out_prefix = args.s.split('/')[-1].split('.')[0]
	out_log = out_prefix + 'log.txt'

	f_log = make_log_file(out_prefix + 'simple_log.txt', p_params = params, p_vars = vars(args))
	pwrite(f_log, 'Starting HMM only mode...' + timer())
	# estimate the number of lines by dividing the total file size by the size of first line
	input_A_signal = fread(args.s)
	input_A_signal.readline() # this is the header line
	input_A_signal.readline() 
	line_size = int(input_A_signal.tell())
	input_A_signal.seek(0,2)
	file_size = int(input_A_signal.tell())
	T_lines = file_size / line_size
	pwrite(f_log, 'Estimated total number of reads in the Tsignal file: ' + str(T_lines))
	n_train_lines = min(max(T_lines/100, params['training_min']), params['training_max'])
	pwrite(f_log, str(n_train_lines) + ' reads picked for training...' + timer())
	input_A_signal.close()
	train_set = lines_sampler(args.s, n_train_lines)

###------------------------------------------------
if args.m:
	# if a pre-trained model is provided, load the model
	F = ghmm.Float()
	model = ghmm.HMMOpenXML(args.m)
	pwrite(f_log, '\nA pre-trained HMM model is provided. No training is carried out. Starting prediction...')
	print(model)
	out_hmm = out_prefix + 'HMM_model.txt' # HMM model
	model.write(out_hmm)
	pi = [1.0, 0.0, 0.0, 0.0, 0.0] # initial state
	
else:
	# if a model is not provided, train a model
	# initializes a gaussian hidden markov model and defines
	# the tranisition, emission, and starting probabilities
	pwrite(f_log, '\nTraining data with hmm...' + timer())
	F = ghmm.Float()

	pi = [1.0, 0.0, 0.0, 0.0, 0.0] # initial state

	if params['allow_back'] == True:
		# The following matrix allows T states going back to non=T states.
		Transitionmatrix = [[0.04, 0.93, 0.02, 0.01, 0.0],
							[0.0, 0.87, 0.1, 0.02, 0.01],
	         	           [0.0, 0.05, 0.6, 0.3, 0.05],
	         	           [0.0, 0.01, 0.3, 0.6, 0.09],
	         	           [0.0, 0.01, 0.01, 0.1, 0.88]]
	else:
		# The following matrix does not allow states going backwards.
		Transitionmatrix = [[0.04, 0.93, 0.02, 0.01, 0.0],
							[0.0, 0.94, 0.03, 0.02, 0.01],
		                    [0.0, 0.0, 0.5, 0.4, 0.1],
		                    [0.0, 0.0, 0.0, 0.6, 0.4],
		                    [0.0, 0.0, 0.0, 0.0, 1.0]]
	# state 0: peudo-A state
	# state 1: definitive-A state
	# state 2: likely-A state
	# state 3: likely-non-A state
	# state 4: definitive-non-A state

	if params['mixed_model'] == True:
		Emissionmatrix = [[[params['bound']*100.0, 0.0], [1.0, 1.0], [1.0, 0.0]],
					  [[1.5, -1.0 ], [1.5, 1.5], [0.95, 0.05]],
	                  [[1.5, -1.0 ], [1.5, 1.5], [0.75, 0.25]],
	                  [[1.5, -1.0 ], [1.5, 1.5], [0.5, 0.5]],
	                  [[1.5, -1.0 ], [1.5, 1.5], [0.25, 0.75]]]
		# [p1_mean, p2,mean], [p1_std, p2_std], [P(p1), P(p2)]
		model = ghmm.HMMFromMatrices(F, ghmm.GaussianMixtureDistribution(F), Transitionmatrix, Emissionmatrix, pi)
	else:
		Emissionmatrix = [[params['bound']*100.0, 1.0],
						  [2.0, 0.5],
		                  [1.0, 0.5],
		                  [-1.0, 0.5],
		                  [-2.0, 0.5]]
		# [mean, std]
		model = ghmm.HMMFromMatrices(F, ghmm.GaussianDistribution(F), Transitionmatrix, Emissionmatrix, pi)

	print('Model before training:')
	print(model)
	mghmm_train = ghmm.SequenceSet(F, train_set)
	model.baumWelch(mghmm_train, 10000, 0.01)
	print('Model after training:')
	print(model)
	out_hmm = out_prefix + 'HMM_model.txt' # output file for HMM model
	model.write(out_hmm)


###------------------------------------------------
# calculate tail length using the mghmm model and write them to output files
# dict_tl structure: {gene_name : [list of tail lengths]}
pwrite(f_log, '\nCalculating tail-lengths and writing outputs...' + timer())

lst_tl = [] # for storing random sequence, tail-length pairs
counting = 0
counting_out = 0
if args.s:
	inA = fread(args.s)
else:
	inA = fread(out_A_signal)
out_header = inA.readline().split('\t\t')[0]

out_states = out_prefix + 'states.txt' # output file for HMM states
out_states = open(out_states, 'w')
out_states.write('id\ttl\thmm_states\n')

out_single_tags = out_prefix + 'pA_all_tags.txt' # output file for all tags with poly(A) tails (by HMM)
out_single_tags = open(out_single_tags, 'w')
out_single_tags.write(out_header + '\t' + 'tail_length' + '\n')
line_lst = []

while(1):
	line = inA.readline()
	if not line:
		with concurrent.futures.ProcessPoolExecutor(params['n_threads']) as pool:
			futures = pool.map(worker_hmm,[line_lst[n:n+params['chunk_lines']] for n in range(0,len(line_lst),params['chunk_lines'])])
			for d in futures: # combine data from outputs from all processes
				for key in d: # write single tail tags to the output file
					out_states.write(key.split('\t')[0]+ '\t' + d[key]['tl'] + '\t' + d[key]['states'] + '\n')
					out_single_tags.write(key + '\t' + d[key]['tl'] + '\n')
					counting_out += 1
			pwrite(f_log, str(counting) + ' reads processed...' + timer())
			break
	else:
		line_lst.append(line)
		counting += 1
		if counting % (params['chunk_lines'] * params['n_threads']) == 0:
			with concurrent.futures.ProcessPoolExecutor(params['n_threads']) as pool:
				futures = pool.map(worker_hmm,[line_lst[n:n+params['chunk_lines']] for n in range(0,len(line_lst),params['chunk_lines'])])
				for d in futures: # combine data from outputs from all processes
					for key in d: # write single tail tags to the output file
						out_states.write(key.split('\t')[0]+ '\t' + d[key]['tl'] + '\t' + d[key]['states'] + '\n')
						out_single_tags.write(key + '\t' + d[key]['tl'] + '\n')
						counting_out += 1
			line_lst = []
		
		if counting % params['chunk'] == 0:
			pwrite(f_log, str(counting) + ' reads processed...' + timer()) 

pwrite(f_log, 'Total number of tail-lengths written: ' + str(counting_out))
pwrite(f_log, 'Finished calculating tail lengths...' + timer())
inA.close()
out_states.close()
out_single_tags.close()
# out_stats file has the following: id, HMM states
# out_single_tags file has the following columns: id, ..., tail length

# delete temporary file containing converted T-signal (very big)
#subprocess.call(['rm','-f',Tsignal_file])











		
		
		
		
		